{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Predicting Time Series - ARIMA & SARIMAX </font>\n",
    "\n",
    "In this module we'll explore two very popular models for time series forecasting - ARIMA and SARIMAX (don't worry if they just sound like pokemon at this point - we'll break down the acronyms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the tractor sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/Tractor-Sales.csv')['Number of Tractor Sold']\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to turn the timeseries stationary by logging and diffing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_logged = sales.map(np.log)\n",
    "sales_logged_diff = sales_logged.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_logged_diff.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> ARIMA </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) is one of the most used models for predicting timeseries. There are more sophisticated versions that we will talk later, but for now we'll use the plain vanilla version to develop some intuitions. \n",
    "\n",
    "ARIMA stands for: \n",
    "\n",
    "- (AR) Auto Regressive \n",
    "- (I)  Integrated \n",
    "- (MA) Moving Average\n",
    "\n",
    "The names are a bit misleading, let's go into them in more detail. \n",
    "\n",
    "About the model API, it elegantly depends only on 3 parameters: \n",
    "\n",
    "> `ARIMA(p,d,q)`\n",
    "\n",
    "Notes: \n",
    "- _The results of our predictions in this notebook will suck, as we are trying to avoid adding complexity, and showcase the API._ \n",
    "- _If you want to try the hardcore version, [this article](https://people.duke.edu/~rnau/411arim.htm) has a very well made explanation for many cases of ARIMA. We are, as mentioned, going to stick to the super-basic \"get your hands dirty\" ARIMA._\n",
    "- _If you are feeling scared with how to select hyper parameters for ARIMA remember that in the real world hyper parameter optimizers can take care of most of this. This notebook shows things as they were in the time where programmers had nothing but a dusty book, a magnetized needle and a steady hand. However getting these intuitions will help you debug and explain your models later._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Auto Regressive </font>\n",
    "\n",
    "The first of our 3 parameters, `p`, is the \"number of auto-regressive terms\". \n",
    "\n",
    "What are auto-regressive terms? They are quite simply the lags of the dependent variable. Is the present point dependent on the previous one? On the previous two? Eight? \n",
    "\n",
    "In plain English, the auto-regressive model says that: \n",
    "> “The value at a particular time depends on the value at the previous times (+ error)” \n",
    "\n",
    "To use this model (we'll use just an AR model, and then add the other components to see the difference), we need to choose the parameter `p`, and use the model as `ARIMA(p, 0, 0)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Choosing the hyper-parameter `p`** </font>\n",
    "\n",
    "Choosing the parameter `p`, as with just about everything in data science, is a mixture of heuristics and experience. \n",
    "\n",
    "Sometimes, you have a strong business reason to say \"I want to only use the previous p lags\". In others, you want to use the data to tell you. \n",
    "\n",
    "Given that in this case we don't have much of a business intuition about tractor sales (at least I don't), we will use the PACF (partial auto-correlation function) to determine a suitable value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(sales_logged_diff, alpha=.05, lags=40, method='ywmle')  \n",
    "sns.mpl.pyplot.xlabel('lag')\n",
    "sns.mpl.pyplot.ylabel('Autocorrelation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us? What we would like to see is a few components clearly go over the significance line. \n",
    "\n",
    "A rule of thumb is to start by testing the number of components where the significance line gets crossed, and make that our `p`. This example was chosen to show that sometimes this doesn't actually happen, and that sometimes these cases are a lot more ambiguous. \n",
    "\n",
    "So we have no guarantees that the data contains an AR process (there is some indication it doesn't), but let's experiment with the ARIMA model to see if we can spot any pattern in the AR part: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.values, order=(1, 0, 0))\n",
    "results = model.fit()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of things to note about the API:\n",
    "1. We passed the data straight to the model (unlike in sklearn) \n",
    "2. We used `.values` to get the numpy array instead of the pandas series\n",
    "3. We passed the order `(p, d, q)` as `(1, 0, 0)`, as we'd established that we want to try `p=1`\n",
    "4. We called `fit` without any parameters (which is also different from sklearn) \n",
    "5. The model isn't fit in place, we have to grab the results with a results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_predict();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Integrated </font>\n",
    "\n",
    "The \"integrated\" part of the name simply means that we take the diff between consecutive periods to make the time series stationary. We've already done this ahead of time (because we needed it for our ACF and PACF plots), but you can also leave it as a hyper parameter and tune it later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, to change the parameter `d`, we follow the same logic as before. Let's set `p` to 1, and try a few values of `d`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parameter_d(data, d):\n",
    "    model = ARIMA(data, order=(1, d, 0))\n",
    "    results = model.fit()\n",
    "    results.plot_predict()\n",
    "    sns.mpl.pyplot.title(f\"ARIMA with d={d}\")\n",
    "    sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that we are passing the original logged data, and letting d diff it as it needs\n",
    "try_parameter_d(sales_logged.astype(float).values, 0)\n",
    "try_parameter_d(sales_logged.astype(float).values, 1)\n",
    "try_parameter_d(sales_logged.astype(float).values, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Moving average </font>\n",
    "\n",
    "The last of our 3 parameters, `q`, is the \"number of moving average terms\". \n",
    "\n",
    "The logic here is similar to the one we used for `p`, but instead of predicting values with lagged values, we are predicting errors with lagged errors. \n",
    "\n",
    "The MA terms are lagged forecast errors. In this model, what predicts `x(t)` is `e(t-1)`, `e(t-2)`, ..., where `e(i)` is the difference between the moving average at the ith instant and the actual value.\n",
    "\n",
    "In plain English, the moving average model says that: \n",
    "> _“your function at a particular time is your error at that time, plus a parameter theta times your error at a previous time”_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Choosing the hyper-parameter `q`** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule of thumb for setting `q` is to use the ACF. The reasons are not trivial, and are well explained in [this fantastic StackExchange post](https://stats.stackexchange.com/questions/281666/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa), if you are interested in digging deeper. \n",
    "\n",
    "If not, just remember: \n",
    "> For `p` use the PACF  \n",
    "> For `q` use the ACF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's plot our ACF: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(sales_logged_diff, alpha=.05, lags=40)  \n",
    "sns.mpl.pyplot.xlabel('lag')\n",
    "sns.mpl.pyplot.ylabel('Autocorrelation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the rule of thumb is \"where we first cross the significance line\", so we'll try q=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.values, order=(0, 0, 1))\n",
    "results = model.fit()\n",
    "results.plot_predict();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Putting the ARIMA components together </font>\n",
    "\n",
    "Great, so now we know how to use the Auto Regressive model, the Moving Average model, and how to control how much differentiation goes on. It only remains to put it all together, and have some fun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.dropna().values, order=(1, 1, 1))\n",
    "results = model.fit()\n",
    "results.plot_predict();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.dropna().values, order=(5, 2, 1))\n",
    "results = model.fit()\n",
    "results.plot_predict();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.dropna().values, order=(0, 1, 0))\n",
    "results = model.fit()\n",
    "results.plot_predict();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Undoing the transformations <font>\n",
    "\n",
    "One last, but critical point, is how to undo our transformations. As you might have noticed, the plots we made don't actually predict sales of tractors, they predict... some weird logged thing differentiated twice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Undoing the diff </font>\n",
    "\n",
    "How can we undo our diff? To take a metaphor from the brilliant [Aileen Nielsen](https://www.youtube.com/watch?v=zmfe2RaX-14), _\"If I tell you my house is 200m shorter than the empire state building, you don't know the height of either\"_. \n",
    "\n",
    "Let's try to undo the diff, with a simple example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([2, 6, 4, 6, 2,])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_diff = a.diff()\n",
    "a_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lost the first term, as we know. Now, let's take the [cumulative sum](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.cumsum.html) of `a_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_diff_cumsum = a_diff.cumsum()\n",
    "a_diff_cumsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's fill that first point with zero, and add the first element of the original series to everything: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt = a_diff_cumsum.fillna(0) + 2\n",
    "rebuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_diffed(series, first_element_original):\n",
    "    \n",
    "    # get the cumulative sum\n",
    "    cumsum = series.cumsum()\n",
    "    \n",
    "    # fill the most recent zero \n",
    "    # (in our case there was just one, but this way it works beyond it)\n",
    "    most_recent_null = cumsum.loc[cumsum.isnull()].index.max()\n",
    "    cumsum.loc[most_recent_null] = 0 \n",
    "    \n",
    "    # return the cumsum, plus the first original element\n",
    "    return cumsum + first_element_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_diffed(a_diff, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Undoing the log  </font>\n",
    "\n",
    "Undoing the log is the easy part, it's simply by taking the exponent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged = a.map(np.log)\n",
    "logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt = logged.apply(np.exp)\n",
    "rebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt == a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!, let's rebuild our own predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_logged_diff.dropna().values, order=(1, 1, 1))\n",
    "results = model.fit();\n",
    "\n",
    "# Get the series of results. These are our un-transformed predictions \n",
    "predictions_after_one_diff_and_a_log = pd.Series(results.predict())\n",
    "\n",
    "# Re-build the logged predictions from the diff of the logged predictions \n",
    "predictions_after_a_log = rebuild_diffed(predictions_after_one_diff_and_a_log, \n",
    "                                                      sales_logged.dropna().iloc[0])\n",
    "\n",
    "# Re-build the predictions from the log of the predictions \n",
    "predictions = predictions_after_a_log.map(np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_after_one_diff_and_a_log.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.plot(label=\"forecast\")\n",
    "sales.plot(label=\"y\")\n",
    "sns.mpl.pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> SARIMAX </font>\n",
    "\n",
    "SARIMAX stands for **Seasonal Autoregressive Integrated Moving Average with Exogeneous Regressors**, and is an evolution of ARIMA, that takes into account seasonality.\n",
    "\n",
    "The **Autoregressive Integrated Moving Average** part we just saw, what about the new bits? \n",
    "\n",
    "- **`Seasonal`**: as the name suggests, this model can actually deal with seasonality. Coool.... \n",
    "- **`With Exogenous`** roughly means we can add external information. For instance we can include the temperature time series to predict the ice cream sales, which is surely useful. Exogenous variables are introduced from or produced outside the organism or system, and don't change with the predictions of the system.\n",
    "\n",
    "What are the parameters? \n",
    "\n",
    "These we already know:\n",
    "> p = 0  \n",
    "> d = 1  \n",
    "> q = 1  \n",
    "\n",
    "But now we have a second bunch. The first 3 are the same as before, but for the seasonal part: \n",
    "> P = 1  \n",
    "> D = 1  \n",
    "> Q = 1  \n",
    "\n",
    "The last new parameter, `S`, is an integer giving the periodicity (number of periods in season). \n",
    "We normally have a decent intuition for this parameter: \n",
    "- If we have daily data and suspect we may have weekly trends, we may want `S` to be 7. \n",
    "- If the data is monthly and we think the time of the year may count, maybe try `S` at 12 \n",
    "> S = 12   \n",
    "\n",
    "To know the SARIMAX in detail you can take a closer look at [the documentation](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the airfline passengers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = utils.load_airline_data()\n",
    "airlines = airlines[:'1957']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(airlines,             \n",
    "                          order=(0, 1, 1),              \n",
    "                          seasonal_order=(1, 1, 1, 12)) # <-- We'll get into how we found these\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "easy to get predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(\n",
    "                            start=airlines.index.min(),  # <--- Start at the first point we have \n",
    "                            dynamic=False)               # <--- predict the last periods without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = pred.predicted_mean\n",
    "mean_predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.plot(label='observed')\n",
    "mean_predictions.plot(label='One-step ahead Forecast with dynamic=False', alpha=.7)\n",
    "sns.mpl.pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ci = pred.conf_int()\n",
    "pred_ci.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matplotlib ugly `fill_between` to plot the confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.plot(label='observed')\n",
    "mean_predictions.plot(label='One-step ahead Forecast with dynamic=False', alpha=.7)\n",
    "\n",
    "# Let's use some matplotlib code to fill between the upper and lower confidence bound with grey \n",
    "sns.mpl.pyplot.fill_between(pred_ci.index, \n",
    "                 pred_ci['lower passengers_thousands'],\n",
    "                 pred_ci['upper passengers_thousands'], \n",
    "                 color='k', \n",
    "                 alpha=.2)\n",
    "\n",
    "sns.mpl.pyplot.ylim([0, 700])\n",
    "sns.mpl.pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of makes sense, at the start we didn't have enough data to predict much, so the uncertaintly band is pretty insane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(series_, pred_):\n",
    "    \n",
    "    \"\"\" \n",
    "    Utility function to plot time series predictions with Confidence intervals\n",
    "    \"\"\"\n",
    "    mean_predictions_ = pred_.predicted_mean\n",
    "    pred_ci_ = pred_.conf_int()   \n",
    "    series_.plot(label='observed')\n",
    "    mean_predictions_.plot(label='predicted', \n",
    "                           alpha=.7)\n",
    "\n",
    "    sns.mpl.pyplot.fill_between(pred_ci_.index,\n",
    "                     pred_ci_['lower passengers_thousands'],\n",
    "                     pred_ci_['upper passengers_thousands'], \n",
    "                     color='k', \n",
    "                     alpha=.2)\n",
    "\n",
    "    sns.mpl.pyplot.ylim([0, 700])\n",
    "    sns.mpl.pyplot.legend()\n",
    "    sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we had stopped feeding it data, and asked it to predict the last 30 periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make 30 steps out of time \n",
    "train_up_to_step = len(airlines) - 30\n",
    "\n",
    "# remember the dynamic argument? Well, we'll use the first 30 steps to train\n",
    "pred = results.get_prediction(start=airlines.index.min(),  \n",
    "                              dynamic=train_up_to_step)                     \n",
    "    \n",
    "plot_predictions(series_=airlines, pred_=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, we can see the uncertainty increasing as we move. Also, what if we wanted to forecast outside of our \"known\" dates?\n",
    "\n",
    "For this we will use the `get_forecast()` method, which allows us to go beyond what data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = results.get_forecast(steps=15)\n",
    "forecast_ci = forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(series_=airlines, pred_=forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
